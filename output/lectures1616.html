<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="text/css" http-equiv="Content-Style-Type"/>
<meta content="pandoc" name="generator"/>
<title></title>
<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
  </style>
</head>
<body>
<!-- <script type="text/javascript" -->
<!--   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> -->
<!-- </script> -->
<script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
<link href="http://www.cs.dartmouth.edu/~scot/cs10/azul.css" rel="stylesheet" type="text/css"/>
<div id="menubar">
<ul>
<li><a href="http://www.cs.dartmouth.edu/~scot/cs10/syllabus.html">Syllabus</a>
<li><a href="http://www.cs.dartmouth.edu/~scot/cs10/schedule.html">Schedule</a>
<li><a href="http://www.cs.dartmouth.edu/~scot/cs10/sa/short_assignments.html">Short assignments</a>
<li><a href="http://www.cs.dartmouth.edu/~scot/cs10/lab/lab_assignments.html">Labs</a>
<li><a href="http://www.cs.dartmouth.edu/~scot/cs10/exams.html">Exams</a>
<li><a href="http://www.cs.dartmouth.edu/~scot/cs10/software.html">Course software</a>
<li><a href="http://www.cs.dartmouth.edu/~scot/cs10/help.html">Get help</a>
</li></li></li></li></li></li></li></ul>
</div>
<div id="termtitle"> CS 10: Fall 2014 </div>
<h1 id="lecture-15-april-25">Lecture 16, October 20</h1>
<h2 id="code-discussed-in-lecture">Code discussed in lecture</h2>
<ul>
<li><a href="WordReduction.java">WordReduction.java</a></li>
</ul>
<h2 id="word-reductions">Word Reductions</h2>
<p>A <strong>word reduction</strong> is a series of words where each word is derived by deleting a single letter of the word before it, until there is only a single letter left. An example would be <em>yeasty</em>, <em>yeast</em>, <em>east</em>, <em>eat</em>, <em>at</em>, <em>a</em>. Any word that appears in such a sequence is called <strong>reducible</strong>. Because a word reduction has to get down to one letter, and the only one-letter words in English are <em>a</em> and <em>I</em>, we can restrict our attention to only those words containing these two letters.</p>
<p>We might like to know all of the reducible works in English, along with the longest such word. The program <a href="WordReduction.java">WordReduction.java</a> is a way to answer these questions. It also demonstrates the use of sets, string manipulation, and file I/O, and so it is a good example for those reasons.</p>
<p>How might we go about solving this problem? Let's call a reducible word a "good" word. If we knew all of the good words shorter than our current word, we could try dropping each letter in the current word and seeing if it is one of the shorter good words. If so, we have found a new good word.</p>
<p>This program reads in all of the words in an online dictionary. The one that I use is one on the Mac, in the file /usr/share/dict/words. There are many other dictionaries online. Because we want to process the words in increasing order of length, we use a list of sets, where the the <span class="math"><em>i</em></span>th list contains a set of words of length <span class="math"><em>i</em> + 1</span>. (No sense in having a list of words of length 0, right?)</p>
<p>We open the file using a <code>FileReader</code> and then use a <code>Scanner</code> to make it easy to process the words. The words in this dictionary appear one per line, and so we read a line, map the word to lower case, get its length, and then add it to the appropriate set.</p>
<p>The method <code>dropLetter</code> demonstrates the use of the <code>substring</code> method on strings. The string positions of string <code>str</code> are indexed from 0 to <code>str.length()-1</code>, just like arrays. We can call the <code>charAt</code> method to select a particular character (which we did in several of the testing programs). To take a substring, we give the start index and the index <em>one beyond the final index</em>. Therefore <code>str.substring(1,2)</code> has length 1 and <code>str.substring(1,1)</code> is the empty string.</p>
<p>We then process the strings in increasing order of length. We add <em>i</em> and <em>a</em> to the set <code>goodWords</code> of good words, and then work our way up from there (see the main method). We also go through and print the good words and the words that they derive from. Finally, we have a recursive method <code>printReduction</code> for printing out a reduction for any given word. Because the output is so large that it blows out the scrolling of the Console pane in Eclipse, we write it out to the file reductions.txt.</p>
<h2 id="hashing">Hashing</h2>
<p>The <code>HashSet</code> and <code>HashMap</code> classes use a data structure called a <strong>hash table</strong>. The idea can be illustrated by how the Sears catalog store in West Lebanon used to keep track of catalog orders to be picked up. They had 100 pigeonholes behind the order desk, numbered 0 to 99. When a customer arrived, the clerk would ask the customers for the last two digits of the customer's phone number. Given a number, the clerk would look in the corresponding pigeonhole for the order form. Even if there were several hundred order forms in the system, each pigeonhole contained only a few forms, and the clerk could search through these quickly. This approach effectively split a set of hundreds of items into 100 sets of a few items each. The trick was to find a hash function that would assign each order form to a particular pigeonhole in a way that spread the order forms fairly evenly among them. The last two digits of the customer's phone number worked well for this purpose. The first two digits would have been a poor choice.</p>
<p>We can do a similar approach inside the computer. For a map, we want to use the key of the (key, value) pair to figure out where we are going. For a set, we use the object itself; in other words, the object <em>is</em> the key. So we'll talk about storing keys in the hash table, possibly with an associated value. In order to implement this idea, we need two things:</p>
<ul>
<li>A table structure in which to store the data. The table is simply a Java array.</li>
<li>A way to get from a key to a particular spot in the table. The way this is done is via a <strong>hash function</strong> <span class="math"><em>h</em></span>, which maps a key <span class="math"><em>k</em></span> to an index <span class="math"><em>h</em>(<em>k</em>)</span> in the table. Ideally, the hash function spreads the objects fairly evenly over the table.</li>
</ul>
<p>Now, if each key mapped to a distinct index in the table, we'd be done. That would be like all Sears customers having the last two digits of their phone numbers be unique. But they are not.</p>
<p>When multiple keys map to the same table index, we have a <strong>collision</strong>. Now, you might think that with 100 slots in the table, we'd have to get close to 100 keys inserted into the table before we'd be likely to have a collision. And you would be incorrect. Have you ever heard of the Birthday Paradox?</p>
<blockquote>
<p>If you randomly select people, how many do you have to select before there is at least a 50% chance that at least two of them have the same birthday?</p>
</blockquote>
<p>Again, you might think that the number is close to 365. Or maybe it's around half of 365: 183? Nope. It's much smaller than that. Much, much smaller. In fact, it's just 23. In terms of hashing, if we have 365 slots in the table and randomly insert keys, once we have inserted 23 keys, we have at least a 50% chance of a collision. Of course, for a 100% chance, we'd have to insert 366 keys.</p>
<p>"Randomly" distributed keys don't do what you might expect.  If you "randomly" distribute <em>n</em> keys into a table of size <em>n</em>, your might expect to find one item in most of the slots, with very few empty slots.  In fact, about <em>n/e</em> of the slots are empty (where <em>e</em> = 2.71..., the base of the natural logarithm).
So over a third of the slots are expected to be empty!  The expected length of the longest 
list is <em>Θ</em>(log <em>n</em>/ log log <em>n</em>).  (To see how to compute these take CS 30.)</p>
<p>So how can we handle collisions? There are a couple of ways. The first one is called <strong>chaining</strong>. Instead of storing each element directly in the table, each slot in the table references a linked list. The linked list for slot <span class="math"><em>i</em></span> holds all the keys <span class="math"><em>k</em></span> for which <span class="math"><em>h</em>(<em>k</em>) = <em>i</em></span>. Here's the idea:</p>
<div class="figure">
<img src="chaining.png"/>
</div>
<p>The keys are <span class="math"><em>k</em><sub>1</sub>, <em>k</em><sub>2</sub>, …, <em>k</em><sub>8</sub></span>. We show each linked list as a noncircular, doubly linked list without a sentinel, and table slots without a linked list are <code>null</code>. Of course, we could make a circular, doubly linked list for each table slot instead, and have each table slot reference one such list, even if the list is empty. In some situations, especially when we only insert into the hash table and never remove elements, singly linked lists suffice.</p>
<p>How many items do we expect to look at when searching for a item? For unsuccessful search (it wasn't in the map or set), we would look at everything in the appropriate list. But how many elements is that? If the table has <span class="math"><em>m</em></span> slots and there are <span class="math"><em>n</em></span> keys stored in it, there would be <span class="math"><em>n</em>/<em>m</em></span> keys per slot on average, and hence <span class="math"><em>n</em>/<em>m</em></span> elements per list on average. We call this ratio, <span class="math"><em>n</em>/<em>m</em></span>, the <strong>load factor</strong>, and we denote it by <span class="math"><em>α</em></span>. If the hash function did a perfect job and distributed the keys perfectly evenly among the slots, then each list has <span class="math"><em>α</em></span> elements. In an unsuccessful search, the average
number of items that we would look at is <em>α</em>.  For a successful search we find the element (so always do 1 comparison), and look at about half of the elements in the list.  This means that for successful search you look at about 1 + <em>α</em>/2
items.  Either way the running time would be <span class="math">Θ(1 + <em>α</em>)</span>. Why "<span class="math">1 + </span>"? Because even if <span class="math"><em>α</em> &lt; 1</span>, we have to account for the time computing the hash function <span class="math"><em>h</em></span>, which we assume to be constant, and for starting the search. (Of course, if <span class="math"><em>α</em> &lt; 1</span>, then we cannot perfectly distribute the keys among all the slots, since a list cannot have a fraction of an element.)</p>
<p>Now, what if the keys are <em>not</em> perfectly distributed? Things get a little trickier, but we operate on the assumption of <strong>simple uniform hashing</strong>, where we assume that any given key is equally likely to hash into any of the <span class="math"><em>m</em></span> slots, without regard to which slot any other key hashed into. When we say "any given key," we mean any <em>possible</em> key, not just those that have been inserted into the hash table. For example, if the keys are strings, then simple uniform hashing says that any string—not just the strings that have been inserted—is equally likely to hash into any slot. Under the assumption of simple uniform hashing, any search, whether successful or not, takes <span class="math">Θ(1 + <em>α</em>)</span> time on average.</p>
<p>Of course, the worst case is bad. It occurs when all keys hash to the same slot. It can happen, even with simple uniform hashing, but of course it's highly unlikely. But the possibility cannot be avoided.  If an adversary puts <em>n*m</em> items into the table then one of the slots will have at least <em>n</em> items in it.  He or she
then makes those <em>n</em> items the data for the problem that you are dealing with, and you are stuck.
(There is an idea called universal hashing, which basically computes a different hash code
every time you run the program, so that data that is slow one time might be fast the next.)</p>
<p>Should the worst case occur, the worst-case time for an unsuccessful search is <span class="math">Θ(<em>n</em>)</span>, since the entire list of <span class="math"><em>n</em></span> elements has to be searched. For a successful search, the worst-case time is still <span class="math">Θ(<em>n</em>)</span>, because the key being searched for could be in the last element in the list.</p>
<p>How about inserting and removing from a hash table with chaining? To insert key <span class="math"><em>k</em></span>, just compute <span class="math"><em>h</em>(<em>k</em>)</span> and insert the key into the linked list for slot <span class="math"><em>h</em>(<em>k</em>)</span>, creating the linked list if necessary. That takes <span class="math">Θ(1)</span> time. How about removing an element? If we assume that we have already searched for it and have a reference to its linked-list node, and that the list is doubly linked, then removing takes <span class="math">Θ(1)</span> time. Again, that's after having paid the price for searching.</p>
<p>Note that if <span class="math"><em>n</em></span> gets much larger than <span class="math"><em>m</em></span>, then search times go up. How can we avoid this? The same way that we do for an <code>ArrayList</code>. When the table gets too full, we create a new one about double the size and rehash everything into the new table. What is "too full"? Java implementations typically start the table with size 11 and double the table size when <span class="math"><em>α</em></span> exceeds 0.75.</p>
<p>Everything is peachy now, right? Yes, except that we are now counting on the table having several empty slots. In other words, we're wasting lots of space, to say nothing of all the links within the lists. If memory is at a premium, as it would be in an embedded system or handheld device, we might regret wasting it.</p>
<p>The second way to handle collisions is called <strong>open addressing</strong>. The idea is to store everything in the table itself, even when collisions occur. There are no linked lists.</p>
<p>How can we store everything in the table even when there's a collision? One simple scheme is called <strong>linear probing</strong>. Suppose we want to insert key <span class="math"><em>k</em></span> and that <span class="math"><em>h</em>(<em>k</em>) = <em>i</em></span>, but slot <span class="math"><em>i</em></span> is already occupied. We cannot put key <span class="math"><em>k</em></span> there. Instead, we probe (look at) slot <span class="math"><em>i</em> + 1</span>. If it's empty, put key <span class="math"><em>k</em></span> there. If slot <span class="math"><em>i</em> + 1</span> is occupied, probe slot <span class="math"><em>i</em> + 2</span>. Keep going, wrapping around to slot 0 after probing slot <span class="math"><em>m</em> − 1</span>. As long as <span class="math"><em>n</em> &lt; <em>m</em></span>, i.e., <span class="math"><em>α</em> &lt; 1</span>, we are guaranteed to eventually find an empty slot. If the table fills, that is, if <span class="math"><em>α</em></span> reaches 1, then increase the table size and rehash everything.</p>
<p>Searching with linear probing uses the same idea as inserting. Compute <span class="math"><em>i</em> = <em>h</em>(<em>k</em>)</span>, and search slots <span class="math"><em>i</em>, <em>i</em> + 1, <em>i</em> + 2, …</span>, wrapping around at slot <span class="math"><em>m</em> − 1</span> until either we find key <span class="math"><em>k</em></span> or we hit an empty slot. If we hit an empty slot, then key <span class="math"><em>k</em></span> was not in the hash table.</p>
<p>Linear probing is a nice idea, but it has a couple of problems. One is how to remove a key from the hash table. We cannot just remove it from its slot. Why not? Let's take the following situation. We insert keys <span class="math"><em>k</em><sub>1</sub></span>, <span class="math"><em>k</em><sub>2</sub></span>, and <span class="math"><em>k</em><sub>3</sub></span>, in that order, where <span class="math"><em>h</em>(<em>k</em><sub>1</sub>) = <em>h</em>(<em>k</em><sub>2</sub>) = <em>h</em>(<em>k</em><sub>3</sub>)</span>. That is, all three keys hash to the same slot, but <span class="math"><em>k</em><sub>1</sub></span> goes into slot <span class="math"><em>i</em></span>, <span class="math"><em>k</em><sub>2</sub></span> goes into slot <span class="math"><em>i</em> + 1</span>, and <span class="math"><em>k</em><sub>3</sub></span> goes into slot <span class="math"><em>i</em> + 2</span>. Then we remove key <span class="math"><em>k</em><sub>2</sub></span>, which opens up a hole in slot <span class="math"><em>i</em> + 1</span>. Then we search for key <span class="math"><em>k</em><sub>3</sub></span>. What's going to happen? We probe slot <span class="math"><em>i</em></span>, see that it holds <span class="math"><em>k</em><sub>1</sub></span>, not <span class="math"><em>k</em><sub>3</sub></span>, and then probe slot <span class="math"><em>i</em> + 1</span>. Because slot <span class="math"><em>i</em> + 1</span> is now empty, we conclude that <span class="math"><em>k</em><sub>3</sub></span> is not in the hash table. Wrong! How can we correct this error? We need some way to mark a slot as having held a key that we have since removed, so that it should be treated as full during a search but as empty during insertion. So if we remove many keys, we can end up with all (or almost all) of the "empty" slots being marked, and unsuccessful searches can go on for a very long time.</p>
<p>But there's another, more insidious, problem. It's called <strong>clustering</strong>. Long runs of occupied slots build up, increasing the average search time. Clusters arise because an empty slot preceded by <span class="math"><em>t</em></span> full slots gets filled next with a probability of <span class="math">(<em>t</em> + 1)/<em>m</em></span>. That's because the probability of hashing to any of the <span class="math"><em>t</em></span> slots is <span class="math">1/<em>m</em></span> (and there are <span class="math"><em>t</em></span> of them), plus the next key could hash directly into that empty slot. And that's not the only way that clusters grow. Clusters can coalesce when one cluster grows and meets with another cluster.</p>
<p>There are various schemes that help a little. One is <strong>double hashing</strong>, where we take two different hash functions <span class="math"><em>h</em><sub>1</sub></span> and <span class="math"><em>h</em><sub>2</sub></span> and make a third hash function <span class="math"><em>h</em>ʹ</span> from them: <span class="math"><em>h</em>'(<em>k</em>,<em>p</em>) = (<em>h</em><sub>1</sub>(<em>k</em>) + <em>ph</em><sub>2</sub>(<em>k</em>)) bmod <em>m</em></span>. The first probe goes to slot <span class="math"><em>h</em><sub>1</sub>(<em>k</em>)</span>, and each successive probe is offset from the previous probe by the amount <span class="math"><em>h</em><sub>2</sub>(<em>k</em>)</span>, all taken modulo <span class="math"><em>m</em></span>. Now, unlike linear or quadratic probing, the probe sequence depends in two ways upon the key, and as long as we design <span class="math"><em>h</em><sub>1</sub></span> and <span class="math"><em>h</em><sub>2</sub></span> so that if <span class="math"><em>h</em><sub>1</sub>(<em>k</em><sub>1</sub>) = <em>h</em><sub>1</sub>(<em>k</em><sub>2</sub>)</span> it's really unlikely that <span class="math"><em>h</em><sub>2</sub>(<em>k</em><sub>1</sub>) = <em>h</em><sub>2</sub>(<em>k</em><sub>2</sub>)</span>, we can avoid  clustering. Again, we must choose our hash functions <span class="math"><em>h</em><sub>1</sub></span> and <span class="math"><em>h</em><sub>2</sub></span> carefully.</p>
<p>When we analyze open addressing, we use the assumption of <strong>uniform hashing</strong>, which says that the probe sequence <span class="math"><em>h</em>ʹ(<em>k</em>, 0), <em>h</em>ʹ(<em>k</em>, 1), <em>h</em>ʹ(<em>k</em>, 2), …, <em>h</em>ʹ(<em>k</em>, <em>m</em> − 1)</span> used when searching or inserting is equally likely to be any permutation of the slot numbers <span class="math">0, 1, 2, …, <em>m</em> − 1</span>. In other words, each possible probe sequence is equally likely.</p>
<p>The result is that the average number of probes in an unsuccessful search or when inserting is approximately <span class="math">1/(1-<em>α</em>)</span>. This number gets very large as <span class="math"><em>α</em></span> approaches 1. If the hash table is 90% full, then about 10 probes are needed. The average number of probes for a successful search isn't so hot, either: <span class="math">-(1/<em>α</em>) log(1/(1-<em>α</em>))</span>. That's a little better than the number of probes for an unsuccessful search; when the hash table is 90% full, it comes to a little over 2.5 probes.</p>
<h2 id="how-to-compute-a-hash-function">How to compute a hash function</h2>
<p>The problem now is how to compute the mapping from keys to spots in a table. This mapping is what the hash function does. A good hash function has three desirable properties:</p>
<ul>
<li>It can be computed quickly.</li>
<li>It spreads the universe of keys fairly evenly over the table.</li>
<li>Small changes in the key (e.g., changing a character in a string or changing the order of the letters) should usually result in a different hash value.</li>
</ul>
<p>Java computes hash functions in two steps. The first is to have every object have a <code>hashCode()</code> method, which returns an int. The default is often the address of the object in memory. Then we have to map this integer into an entry in the table. This is called the compression function.</p>
<p>A simple compression function is to take the hashcode modulo the table size, as we did above. This scheme works well when the table size is a prime number, but not so well otherwise. In that case, the book suggests computing <span class="math">((<em>ai</em> + <em>b</em>) mod q) mod m</span>, where <span class="math"><em>i</em></span> is the hashcode, <span class="math"><em>q</em></span> is a prime greater than <span class="math"><em>m</em></span>, and <span class="math"><em>a</em></span> and <span class="math"><em>b</em></span> are chosen at random between 1 and <span class="math"><em>q</em> − 1</span>. (We can also let <span class="math"><em>b</em></span> be 0.) Fortunately for us, Java's library takes care of the compression function.</p>
<p>Java gives us the responsibility to come up with a good hashcode, however. In particular, if you define <code>equals</code> for an object, it is very important that you override <code>hashCode</code> so that two items considered equal have the same hashcode. That's because if you insert an item in the hash table and then search for it using something that is equal to the item, then you expect to find it. If the hashcodes are different you won't find it. You will be looking in the wrong slot.</p>
<p>So how can you compute hashcodes, particularly for composite objects (objects that have several instance variables or are strings or arrays, etc.)? A simple option is to sum all of the pieces, or sum the hashcodes of the pieces. But there's a problem with this approach: shuffling the order (say of a string) does not change the hashcode. It probably should! A better idea for computing the hashcode of what we can view as a tuple <span class="math">(<em>x</em><sub>0</sub>, <em>x</em><sub>1</sub>, …, <em>x</em><sub><em>s</em> − 1</sub>)</span> is to pick a positive number <span class="math"><em>a</em></span> (the book suggests that 33, 37, 39, and 41 are good choices) and compute the polynomial <span class="math"><em>x</em><sub>0</sub><em>a</em><sup><em>s</em> − 1</sup> + <em>x</em><sub>1</sub><em>a</em><sup><em>s</em> − 2</sup> + <em>x</em><sub>3</sub><em>a</em><sup><em>s</em> − 2</sup> + ⋯ + <em>x</em><sub>0</sub></span>. The book shows how to do this using Horner's rule, which basically means starting with a running total at <span class="math"><em>x</em><sub>0</sub></span> and for <span class="math"><em>j</em> = 1, 2, …, <em>s</em> − 1</span>, multiplying by <span class="math"><em>a</em></span> and adding <span class="math"><em>x</em><sub><em>j</em></sub></span>:</p>
<pre class="sourceCode java"><code class="sourceCode java"><span class="kw">public</span> <span class="dt">int</span> <span class="fu">hashcode</span>() {
  <span class="dt">final</span> <span class="dt">int</span> a = <span class="dv">37</span>;
  
  <span class="dt">int</span> sum = x[<span class="dv">0</span>];
  <span class="kw">for</span> (<span class="dt">int</span> j = <span class="dv">1</span>; j &lt; s; j++)
    sum = a * sum + x[j];
    
  <span class="kw">return</span> sum;
}</code></pre>
<p>The book also shows a way to use cyclic shifts for this purpose. For the curious, there is a lot of literature on hashing, including Donald Knuth's <em>The Art of Computer Programming</em>, Volume 3. (In my opinion, Knuth's multivolume set, <em>The Art of Computer Programming</em>, is outstanding.)</p>
</body>
</html>

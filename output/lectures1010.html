<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="text/css" http-equiv="Content-Style-Type"/>
<meta content="pandoc" name="generator"/>
<title></title>
<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
  </style>
</head>
<body>
<!-- <script type="text/javascript" -->
<!--   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> -->
<!-- </script> -->
<script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
<link href="http://www.cs.dartmouth.edu/~scot/cs10/azul.css" rel="stylesheet" type="text/css"/>
<div id="menubar">
<ul>
<li><a href="http://www.cs.dartmouth.edu/~scot/cs10/syllabus.html">Syllabus</a>
<li><a href="http://www.cs.dartmouth.edu/~scot/cs10/schedule.html">Schedule</a>
<li><a href="http://www.cs.dartmouth.edu/~scot/cs10/sa/short_assignments.html">Short assignments</a>
<li><a href="http://www.cs.dartmouth.edu/~scot/cs10/lab/lab_assignments.html">Labs</a>
<li><a href="http://www.cs.dartmouth.edu/~scot/cs10/exams.html">Exams</a>
<li><a href="http://www.cs.dartmouth.edu/~scot/cs10/software.html">Course software</a>
<li><a href="http://www.cs.dartmouth.edu/~scot/cs10/help.html">Get help</a>
</li></li></li></li></li></li></li></ul>
</div>
<div id="termtitle"> CS 10: Fall 2014 </div>
<h1 id="lecture-11-april-16">Lecture 10, October 6</h1>
<h2 id="important-organizational-notes">Important organizational notes</h2>
<p>There will no be no class this Friday (April 18). Also, office hours for Thursday, April 17 are canceled.</p>
<h2 id="code-discussed-in-lecture">Code discussed in lecture</h2>
<ul>
<li><a href="CS10Stack.java">CS10Stack.java</a></li>
<li><a href="ArrayListStack.java">ArrayListStack.java</a></li>
<li><a href="CS10Queue.java">CS10Queue.java</a></li>
<li><a href="LinkedListQueue.java">LinkedListQueue.java</a></li>
</ul>
<h2 id="analysis-of-algorithms">Analysis of algorithms</h2>
<p>You have probably seen big-Oh notation before, but it's certainly worthwhile to recap it. In addition, we'll see a couple of other related <strong>asymptotic notations</strong>. Chapter 4 of the textbook covers this material as well.
I strongly suggest reading it.</p>
<h3 id="order-of-growth">Order of growth</h3>
<p>Remember back to linear search and binary search. Both are algorithms to search for a value in an array with <span class="math"><em>n</em></span> elements. Linear search marches through the array, from index 0 through the highest index, until either the value is found in the array or we run off the end of the array. Binary search, which requires the array to be sorted, repeatedly discards half of the remaining array from consideration, considering subarrays of size <span class="math"><em>n</em>, <em>n</em>/2, <em>n</em>/4, <em>n</em>/8, …, 1, 0</span> until either the value is found in the array or the size of the remaining subarray under consideration is 0.</p>
<p>The worst case for linear search arises when the value being searched for is not present in the array. The algorithm examines all <span class="math"><em>n</em></span> positions in the array. If each test takes a constant amount of time—that is, the time per test is a constant, independent of <span class="math"><em>n</em></span>—then linear search takes time <span class="math"><em>c</em><sub>1</sub><em>n</em> + <em>c</em><sub>2</sub></span>, for some constants <span class="math"><em>c</em><sub>1</sub></span> and <span class="math"><em>c</em><sub>2</sub></span>. The additive term <span class="math"><em>c</em><sub>2</sub></span> reflects the work done before and after the main loop of linear search. Binary search, on the other hand, takes <span class="math"><em>c</em><sub>3</sub> log<sub>2</sub> <em>n</em> + <em>c</em><sub>4</sub></span> time in the worst case, for some constants <span class="math"><em>c</em><sub>3</sub></span> and <span class="math"><em>c</em><sub>4</sub></span>. (Recall that when we repeatedly halve the size of the remaining array, after at most <span class="math">log<sub>2</sub> <em>n</em> + 1</span> halvings, we've gotten the size down to 1.) Base-2 logarithms arise so frequently in computer science that we have a notation for them: <span class="math">lg <em>n</em> = log<sub>2</sub> <em>n</em></span>.</p>
<p>Where linear search has a linear term, binary search has a logarithmic term. Recall that <span class="math">lg <em>n</em></span> grows <em>much</em> more slowly than <span class="math"><em>n</em></span>; for example, when <span class="math"><em>n</em></span> = 1,000,000,000 (a billion), <span class="math">lg <em>n</em></span> is approximately 30.</p>
<p>If we consider only the leading terms and ignore the coefficients for running times, we can say that in the worst case, linear search's running time "grows like" <span class="math"><em>n</em></span> and binary search's running time "grows like" <span class="math">lg <em>n</em></span>. This notion of "grows like" is the essence of the running time. Computer scientists use it so frequently that we have a special notation for it: "big-Oh" notation, which we write as "<span class="math"><em>O</em></span>-notation."</p>
<p>For example, the running time of linear search is always at most some linear function of the input size <span class="math"><em>n</em></span>. Ignoring the coefficients and low-order terms, we write that the running time of linear search is <span class="math"><em>O</em>(<em>n</em>)</span>. You can read the <span class="math"><em>O</em></span>-notation as "order." In other words, <span class="math"><em>O</em>(<em>n</em>)</span> is read as "order <span class="math"><em>n</em></span>." You'll also hear it spoken as "big-Oh of <span class="math"><em>n</em></span>" or even just "Oh of <span class="math"><em>n</em></span>."</p>
<p>Similarly, the running time of binary search is always at most some logarithmic function of the input size <span class="math"><em>n</em></span>. Again ignoring the coefficients and low-order terms, we write that the running time of binary search is <span class="math"><em>O</em>(lg <em>n</em>)</span>, which we would say as "order log <span class="math"><em>n</em></span>," "big-Oh of log <span class="math"><em>n</em></span>," or "Oh of log <span class="math"><em>n</em></span>."</p>
<p>In fact, within our <span class="math"><em>O</em></span>-notation, if the base of a logarithm is a constant (such as 2), then it doesn't really matter. That's because of the formula</p>
<blockquote>
<p><span class="math">log<sub><em>a</em></sub> <em>n</em> = log<sub><em>b</em></sub> <em>n</em> / log<sub><em>b</em></sub> <em>a</em></span></p>
</blockquote>
<p>for all positive real numbers <span class="math"><em>a</em></span>, <span class="math"><em>b</em></span>, and <span class="math"><em>c</em></span>. In other words, if we compare <span class="math">log<sub><em>a</em></sub> <em>n</em></span> and <span class="math">log<sub><em>b</em></sub> <em>n</em></span>, they differ by a factor of <span class="math">log<sub><em>b</em></sub> <em>a</em></span>, and this factor is a constant if <span class="math"><em>a</em></span> and <span class="math"><em>b</em></span> are constants. Therefore, even when we use the "<span class="math">lg</span>" notation within <span class="math"><em>O</em></span>-notation, it's irrelevant that we're really using base-2 logarithms.</p>
<p><span class="math"><em>O</em></span>-notation is used for what we call "asymptotic upper bounds." By "asymptotic" we mean "as the argument (<span class="math"><em>n</em></span>) gets large." By "upper bound" we mean that <span class="math"><em>O</em></span>-notation gives us a bound from above on how high the rate of growth is.</p>
<p>Here's the technical definition of <span class="math"><em>O</em></span>-notation, which will underscore both the "asymptotic" and "upper-bound" notions:</p>
<blockquote>
<p>A running time is <span class="math"><em>O</em>(<em>n</em>)</span> if there exist positive constants <span class="math"><em>n</em><sub>0</sub></span> and <span class="math"><em>c</em></span> such that for all problem sizes <span class="math"><em>n</em> ≥ <em>n</em><sub>0</sub></span>, the running time for a problem of size <span class="math"><em>n</em></span> is at most <span class="math"><em>c</em><em>n</em></span>.</p>
</blockquote>
<p>Here's a helpful picture:</p>
<div class="figure">
<img src="order-n.png"/>
</div>
<p>The "asymptotic" part comes from our requirement that we care only about what happens at or to the right of <span class="math"><em>n</em><sub>0</sub></span>, i.e., when <span class="math"><em>n</em></span> is large. The "upper bound" part comes from the running time being <em>at most</em> <span class="math"><em>c</em><em>n</em></span>. The running time can be less than <span class="math"><em>c</em><em>n</em></span>, and it can even be a lot less. What we require is that there exists some constant <span class="math"><em>c</em></span> such that for sufficiently large <span class="math"><em>n</em></span>, the running time is bounded from above by <span class="math"><em>c</em><em>n</em></span>.</p>
<p>For an arbitrary function <span class="math"><em>f</em>(<em>n</em>)</span>, which is not necessarily linear, we extend our technical definition:</p>
<blockquote>
<p>A running time is <span class="math"><em>O</em>(<em>f</em>(<em>n</em>))</span> if there exist positive constants <span class="math"><em>n</em><sub>0</sub></span> and <span class="math"><em>c</em></span> such that for all problem sizes <span class="math"><em>n</em> ≥ <em>n</em><sub>0</sub></span>, the running time for a problem of size <span class="math"><em>n</em></span> is at most <span class="math"><em>c</em> <em>f</em>(<em>n</em>)</span>.</p>
</blockquote>
<p>A picture:</p>
<div class="figure">
<img src="order-fn.png"/>
</div>
<p>Now we require that there exist some constant <span class="math"><em>c</em></span> such that for sufficiently large <span class="math"><em>n</em></span>, the running time is bounded from above by <span class="math"><em>c</em> <em>f</em>(<em>n</em>)</span></p>
<p>Actually, <span class="math"><em>O</em></span>-notation applies to functions, not just to running times. But since our running times will be expressed as functions of the input size <span class="math"><em>n</em></span>, we can express running times using <span class="math"><em>O</em></span>-notation.</p>
<p>In general, we want as <strong>slow</strong> a rate of growth as possible, since if the running time grows slowly, that means that the algorithm is relatively fast for larger problem sizes.</p>
<p>We usually focus on the <strong>worst case</strong> running time, for several reasons:</p>
<ul>
<li>The worst case time gives us an upper bound on the time required for <strong>any</strong> input.</li>
<li>It gives a <strong>guarantee</strong> that the algorithm <strong>never</strong> takes any longer.</li>
<li>We don't need to make an educated guess and hope that the running time never gets much worse.</li>
</ul>
<p>You might think that it would make sense to focus on the "average case" rather than the worst case, which is exceptional. And sometimes we do focus on the average case. But often it makes little sense. First, you have to determine just what <em>is</em> the average case for the problem at hand. Suppose we're searching. In some situations, you find what you're looking for early. For example, a video database will put the titles most often viewed where a search will find them quickly. In some situations, you find what you're looking for on average halfway through all the data…for example, a linear search with all search values equally likely. In some situations, you usually don't find what you're looking for.</p>
<p>It is also often true that the average case is about as bad as the worst case. Because the worst case is usually easier to identify than the average case, we focus on the worst case.</p>
<p>Computer scientists use notations analogous to <span class="math"><em>O</em></span>-notation for "asymptotic lower bounds" (i.e., the running time grows <em>at least</em> this fast) and "asymptotically tight bounds" (i.e., the running time is <em>within a constant factor</em> of some function). We use <span class="math">Ω</span>-notation (that's the Greek leter "omega") to say that the function grows "at least this fast". It is almost the same as Big-Oh notation, except that is has an "at least" instead of an "at most":</p>
<blockquote>
<p>A running time is <span class="math">Ω(<em>f</em>(<em>n</em>))</span> if there exist positive constants <span class="math"><em>n</em><sub>0</sub></span> and <span class="math"><em>c</em></span> such that for all problem sizes <span class="math"><em>n</em> ≥ <em>n</em><sub>0</sub></span>, the running time for a problem of size <span class="math"><em>n</em></span> is at least <span class="math"><em>c</em> <em>f</em>(<em>n</em>)</span>.</p>
</blockquote>
<p>We use <span class="math">Θ</span>-notation (that's the Greek letter "theta") for asymptotically tight bounds:</p>
<blockquote>
<p>A running time is <span class="math">Θ(<em>f</em>(<em>n</em>))</span> if there exist positive constants <span class="math"><em>n</em><sub>0</sub></span>, <span class="math"><em>c</em><sub>1</sub></span>, and <span class="math"><em>c</em><sub>2</sub></span> such that for all problem sizes <span class="math"><em>n</em> ≥ <em>n</em><sub>0</sub></span>, the running time for a problem of size <span class="math"><em>n</em></span> is at least <span class="math"><em>c</em><sub>1</sub> <em>f</em>(<em>n</em>)</span> and at most <span class="math"><em>c</em><sub>2</sub> <em>f</em>(<em>n</em>)</span>.</p>
</blockquote>
<p>Pictorially,</p>
<div class="figure">
<img src="Theta-fn.png"/>
</div>
<p>In other words, with <span class="math">Θ</span>-notation, for sufficiently large problem sizes, we have nailed the running time to within a constant factor. As with <span class="math"><em>O</em></span>-notation, we can ignore low-order terms and constant coefficients in <span class="math">Θ</span>-notation.</p>
<p>Note that <span class="math">Θ</span>-notation subsumes <span class="math"><em>O</em></span>-notation in that</p>
<blockquote>
<p>If a running time is <span class="math">Θ(<em>f</em>(<em>n</em>))</span>, then it is also <span class="math"><em>O</em>(<em>f</em>(<em>n</em>))</span>.</p>
</blockquote>
<p>The converse (<span class="math"><em>O</em>(<em>f</em>(<em>n</em>))</span> implies <span class="math">Θ(<em>f</em>(<em>n</em>))</span>) does not necessarily hold.</p>
<p>The general term that we use for <span class="math"><em>O</em></span>-notation, <span class="math">Θ</span>-notation, and <span class="math">Ω</span>-notation is <strong>asymptotic notation</strong>.</p>
<h3 id="recap">Recap</h3>
<p>Asymptotic notations provide ways to characterize the <strong>rate of growth</strong> of a function <span class="math"><em>f</em>(<em>n</em>)</span>. For our purposes, the function <span class="math"><em>f</em>(<em>n</em>)</span> describes the running time of an algorithm, but it really could be any old function. Asymptotic notation describes what happens as <span class="math"><em>n</em></span> gets large; we don't care about small values of <span class="math"><em>n</em></span>. We use <span class="math"><em>O</em></span>-notation to bound the rate of growth from above to within a constant factor, and we use <span class="math">Θ</span>-notation to bound the rate of growth to within constant factors from both above and below. (We won't use <span class="math">Ω</span>-notation much in this course.)</p>
<p>We need to understand when we can apply each asymptotic notation. For example, in the worst case, linear search runs in time proportional to the input size <span class="math"><em>n</em></span>; we can say that linear search's worst-case running time is <span class="math">Θ(<em>n</em>)</span>. It would also be correct, but slightly less precise, to say that linear search's worst-case running time is <span class="math"><em>O</em>(<em>n</em>)</span>. Because in the best case, linear search finds what it's looking for in the first array position it checks, we cannot say that linear search's running time is <span class="math">Θ(<em>n</em>)</span> in <em>all</em> cases. But we <em>can</em> say that linear search's running time is <span class="math"><em>O</em>(<em>n</em>)</span> in <em>all</em> cases, since it never takes longer than some constant times the input size <span class="math"><em>n</em></span>.</p>
<h3 id="working-with-asymptotic-notation">Working with asymptotic notation</h3>
<p>Although the definitions of <span class="math"><em>O</em></span>-notation an <span class="math">Θ</span>-notation may seem a bit daunting, these notations actually make our lives easier in practice. There are two ways in which they simplify our lives.</p>
<p>I won't go through the math that follows in class. You may read it, in the context of the formal definitions of <span class="math"><em>O</em></span>-notation and <span class="math">Θ</span>-notation, if you wish. For now, the main thing is to get comfortable with the ways that asymptotic notation makes working with a function's rate of growth easier.</p>
<h4 id="constant-factors-dont-matter">Constant factors don't matter</h4>
<p>Constant multiplicative factors are "absorbed" by the multiplicative constants in <span class="math"><em>O</em></span>-notation (<span class="math"><em>c</em></span>) and <span class="math">Θ</span>-notation (<span class="math"><em>c</em><sub>1</sub></span> and <span class="math"><em>c</em><sub>2</sub></span>). For example, the function <span class="math">1000 <em>n</em><sup>2</sup></span> is <span class="math">Θ(<em>n</em><sup>2</sup>)</span> since we can choose both <span class="math"><em>c</em><sub>1</sub></span> and <span class="math"><em>c</em><sub>2</sub></span> to be <span class="math">1000</span>.</p>
<p>Although we may care about constant multiplicative factors in practice, we focus on the rate of growth when we analyze algorithms, and the constant factors don't matter. Asymptotic notation is a great way to suppress constant factors.</p>
<h4 id="low-order-terms-dont-matter-either">Low-order terms don't matter, either</h4>
<p>When we add or subtract low-order terms, they disappear when using asymptotic notation. For example, consider the function <span class="math"><em>n</em><sup>2</sup> + 1000 <em>n</em></span>. I claim that this function is <span class="math">Θ(<em>n</em><sup>2</sup>)</span>. Clearly, if I choose <span class="math"><em>c</em><sub>1</sub> = 1</span>, then I have <span class="math"><em>n</em><sup>2</sup> + 1000 <em>n</em> ≥ <em>c</em><sub>1</sub> <em>n</em><sup>2</sup></span>, and so this side of the inequality is taken care of.</p>
<p>The other side is a bit tougher. I need to find a constant <span class="math"><em>c</em><sub>2</sub></span> such that for sufficiently large <span class="math"><em>n</em></span>, I'll get that <span class="math"><em>n</em><sup>2</sup> + 1000 <em>n</em> ≤ <em>c</em><sub>2</sub> <em>n</em><sup>2</sup></span>. Subtracting <span class="math"><em>n</em><sup>2</sup></span> from both sides gives <span class="math">1000 <em>n</em> ≤ <em>c</em><sub>2</sub> <em>n</em><sup>2</sup> − <em>n</em><sup>2</sup> = (<em>c</em><sub>2</sub> − 1) <em>n</em><sup>2</sup></span>. Dividing both sides by <span class="math">(<em>c</em><sub>2</sub> − 1) <em>n</em></span> gives <span class="math">$\displaystyle \frac{1000}{c_2 - 1} \leq n$</span>. Now I have some flexibility, which I'll use as follows. I pick <span class="math"><em>c</em><sub>2</sub> = 2</span>, so that the inequality becomes <span class="math">$\displaystyle \frac{1000}{2-1} \leq n$</span>, or <span class="math">1000 ≤ <em>n</em></span>. Now I'm in good shape, because I have shown that if I choose <span class="math"><em>n</em><sub>0</sub> = 1000</span> and <span class="math"><em>c</em><sub>2</sub> = 2</span>, then for all <span class="math"><em>n</em> ≥ <em>n</em><sub>0</sub></span>, I have <span class="math">1000 ≤ <em>n</em></span>, which we saw is equivalent to <span class="math"><em>n</em><sup>2</sup> + 1000 <em>n</em> ≤ <em>c</em><sub>2</sub> <em>n</em><sup>2</sup></span>.</p>
<p>The point of this example is to show that adding or subtracting low-order terms just changes the <span class="math"><em>n</em><sub>0</sub></span> that we use. In our practical use of asymptotic notation, we can just drop low-order terms.</p>
<h4 id="combining-them">Combining them</h4>
<p>In combination, constant factors and low-order terms don't matter. If we see a function like <span class="math">1000 <em>n</em><sup>2</sup> − 200 <em>n</em></span>, we can ignore the low-order term <span class="math">200 <em>n</em></span> and the constant factor <span class="math">1000</span>, and therefore we can say that <span class="math">1000 <em>n</em><sup>2</sup> − 200 <em>n</em></span> is <span class="math">Θ(<em>n</em><sup>2</sup>)</span>.</p>
<h3 id="when-to-use-o-notation-vs.-theta-notation">When to use <span class="math"><em>O</em></span>-notation vs. <span class="math">Θ</span>-notation</h3>
<p>As we have seen, we use <span class="math"><em>O</em></span>-notation for asymptotic upper bounds and <span class="math">Θ</span>-notation for asymptotically tight bounds. <span class="math">Θ</span>-notation is more precise than <span class="math"><em>O</em></span>-notation. Therefore, we prefer to use <span class="math">Θ</span>-notation whenever it's appropriate to do so.</p>
<p>We shall see times, however, in which we cannot say that a running time is tight to within a constant factor both above and below. Sometimes, we can bound a running time only from above. In other words, we might only be able to say that the running time is no worse than a certain function of <span class="math"><em>n</em></span>, <em>but it might be better</em>. In such cases, we'll have to use <span class="math"><em>O</em></span>-notation, which is perfect for such situations.</p>
<h3>Predicting running times</h3>
<p>Big <span class="math"><em>Θ</em></span>-notation can help predict running times for
big data sets based on the run times for small data sets. Suppose you have a program that runs the insertion sort algorithm, whose
worst and average case run times are Θ(<i>n</i><sup>2</sup>), where <i>n</i> is the 
number of items to sort.  Someone has given
you a list of 10,000 randomly ordered things to sort.  Your program takes 0.1
seconds.  Then you are given a list of 1,000,000 randomly ordered things to
sort.  How long do you expect your program to take?  Should you wait?
Get coffee?  Go to dinner?  Go to bed? How can you decide?</p>
<p>The expected run time for insertion sort on random data is Θ(<i>n</i><sup>2</sup>).
The means that the run time approaches <i>cn</i><sup>2</sup> 
asymptotically, or is at least upper-bounded by such an expression.  10,000 is
fairly big - the lower-order terms shouldn't have much effect.  Therefore we approximate
the actual function for the run time by <i>f</i>(<i>n</i>) = <i>cn</i><sup>2</sup>.  </p>
<p>There are two ways we can solve this.  The more straighforward way (and the way that will
work for run times like Θ(<i>n</i> log <i>n</i>)) is to solve for <i>c</i>:</p>
<p>
<i>f</i>(10,000) =  <i>c</i> 10,000<sup>2</sup> = 0.1 <br>
<i>c</i> = 0.1/(10<sup>4</sup>)<sup>2</sup> = 10<sup>-9</sup><br>
</br></br></p>
<p>Therefore we can approximate  <i>f</i>(<i>n</i>) as:

<p><i>f</i>(<i>n</i>) = 10<sup>-9</sup><i>n</i><sup>2</sup></p>
<p>Plugging in <i>n</i> = 1,000,000 we get:</p>
<p><i>f</i>(1,000,000) = 10<sup>-9</sup> (10<sup>6</sup>)<sup>2</sup> = 1000</p>
<p>We therefore estimate 1000 seconds or a 16.666 minutes.  Time enough to get a cup of coffee,
but not to go to dinner.</p>
<p>We can use a shortcut for polynomials.  If the run time is approximately <i>cn</i><sup><i>k</i></sup>
for some <i>k</i>, then what happens to the run time if we change the input size by a factor of <i>p</i>?
That is, the old input size is <i>n</i> and the new input size is <i>pn</i>.  Thus if 

<p><i>f</i>(<i>n</i>) = <i>cn</i><sup><i>k</i></sup></p>

then 

<p><i>f</i>(<i>pn</i>) = <i>c(pn)</i><sup><i>k</i></sup> = <i>p</i><sup><i>k</i></sup><i>cn</i><sup><i>k</i></sup> =
<i>p</i><sup><i>k</i></sup><i>f</i>(<i>n</i>)</p>
<p>Therefore changing the input size by a factor of <i>p</i> changes the run time by a factor of <i>p</i><sup><i>k</i></sup>.
In the problem I gave you, <i>p</i> = 1,000,000/10,000 = 100 and <i>k</i> = 2, so the run time changes by a factor
of 10,000.  So the time is 10,000 times 0.1, or 1000 seconds.</p>
<p>Suppose that you were running mergesort instead of insertion sort in the example above. Merge sort's run time is
 Θ(<i>n</i> log <i>n</i>).  We approximate the run time as <i>f</i>(<i>n</i>) = <i>cn</i> log <i>n</i>.  Note that
log<sub>2</sub>10,000 = 13.29. Solving for <i>c</i> gives:</p>
<p><i>f</i>(10,000) =  <i>c</i> 10,000 log 10,000 = <i>c</i> 10,000 (13.29) = 0.1 <br>
<i>c</i> = 0.1/132,900 = 7.52 x 10<sup>-7</sup><br>
<i>f</i>(<i>n</i>) = (7.53 x 10<sup>-7</sup>) n log n  </br></br></p>
<p>Plugging in <i>n</i> = 1,000,000  and using the fact that log 1,000,000 is a bit less than 20 gives: </p>
<i>f</i>(1,000,000) = (7.53 x 10<sup>-7</sup>) 1,000,000 log 1,000,000 = .753 x 20 = 15.05 </p>
</p>So the run time in this case is 15 seconds. Might as well wait for it to finish.</body></html>
<h2 id="stacks">Stacks</h2>
<p>A stack is a LIFO (Last In, First Out) data structure. The book compares a stack to a spring-loaded stack of cafeteria trays or a PEZ dispenser. (Though I have to question how well the authors remember PEZ. They are candies, but definitely not mint.) The abstract data type (ADT) <code>Stack</code> has at least the following operations (in addition to a constructor to create an empty stack):</p>
<ul>
<li><code>push</code>: add to the top of the stack</li>
<li><code>pop</code>: remove the top element from the stack and return it</li>
<li><code>peek</code>: return the top element but do not remove it</li>
<li><code>isEmpty</code>: return a boolean indicating whether the stack is empty</li>
</ul>
<p>So what good is a stack? It has many, many applications. You already know that the run-time stack handles allocating memory in method calls and freeing memory on returns. (It also allows recursion). A stack provides an easy way to reverse a list or string: push each element on the stack then pop them all off. They come off in the opposite order that they went on. They are good for matching parentheses or braces or square brackets or open and close tags in HTML.</p>
<p>A stack is also how HP calculators handle reverse Polish notation. In this notation the operator follows both operands, and no parentheses are needed. So the expression <code>(3 + 5) * (4 - 6)</code> becomes <code>3 5 + 4 6 - *</code>. To evaluate it, push operands onto the stack when you encounter them. When you reach an operator, pop the top two values from the stack and apply the operator to them. (The first popped becomes the second operand in the operation.) Push the result of the operation back on the stack. At the end there is a single value on the stack, and that is the value of the expression.</p>
<p>A stack is also how you can do depth-first search of a maze or a graph. Let's consider a maze. Start by pushing the start square onto the stack. The repeatedly do the following:</p>
<ol style="list-style-type: decimal">
<li>Pop the stack to get the next square to visit.</li>
<li>Push all unvisited adjacent squares onto the stack.</li>
</ol>
<p>Quit when you reach the goal square.</p>
<!--(Demonstrate how the search works using the Maze application.)-->
<h3 id="implementing-a-stack">Implementing a stack</h3>
<p>Because <code>Stack</code> is an ADT, an interface should specify its operations. The <code>CS10Stack</code> interface in <a href="CS10Stack.java">CS10Stack.java</a> contains the operations given above.</p>
<p>The class <code>java.util.Stack</code> also has these operations, but instead of the name <code>isEmpty</code> they use <code>empty</code>. It also has an additional operation, <code>size</code>.</p>
<p>The book has its own version of the ADT, but instead of the name <code>peek</code> they use <code>top</code>, and they also add <code>size</code>. You would think that computer scientists could agree on a standard set of names. Yeah, not so much. At least we all agree on <code>push</code> and <code>pop</code>.</p>
<p>One question is how to handle <code>pop</code> or <code>peek</code> on an empty stack. Both Java and the book throw an exception. That seems a bit harsh, and so <code>CS10Stack</code> is more forgiving: it returns <code>null</code>.</p>
<p>How do we implement a stack? One simple option is to use an array. The implementation has two instance variables: an array called <code>stack</code> and an int called <code>top</code> that keeps track of the position of the top of the stack. In an empty stack <code>top</code> equals <span class="math"> − 1</span>. To push, add 1 to <code>top</code> and save the value pushed in <code>stack[top]</code>. To <code>peek</code> just return <code>stack[top]</code> (after checking that <code>top</code> is nonnegative). <code>pop</code> is <code>peek</code> but with <code>top--</code>.</p>
<p>This implementation is fast (all operations take <span class="math"><em>O</em>(1)</span> time), and it is space efficient (except for the unused part of the array). The drawback is that the array can fill up, and when it does, you get an exception on <code>push</code>.</p>
<p>An alternative that avoids that problem uses a linked list. A singly linked list suffices. The top of the stack is the head of the list. The push operation adds to the front of the list, and the pop removes from the front of the list. All operations are take <span class="math"><em>O</em>(1)</span> time in this implementation, also. You need to have space for the links in the linked list, but you never have empty space as you do in the array implementation.</p>
<p>Another way that avoids the problem of the array being full is to use an <code>ArrayList</code>. To push, you add to the end of the <code>ArrayList</code>, and to pop you remove the last element. The <code>ArrayList</code> can grow, so it never becomes full. The code for this implementation is in <a href="ArrayListStack.java">ArrayListStack.java</a>. Note that you don't even need to keep track of the top. The <code>ArrayList</code> does it for you.</p>
<p>Do these operations all take <span class="math"><em>O</em>(1)</span> time? It looks like it, as long as <code>add</code> and <code>remove</code> at the end of the <code>ArrayList</code> take <span class="math"><em>O</em>(1)</span> time. The <code>remove</code> operation certainly takes <span class="math"><em>O</em>(1)</span> time. The <code>add</code> usually does is, but sometimes can take longer.</p>
<p>To understand why an <code>add</code> operation can take more than constant time, we need to look at how an <code>ArrayList</code> is implemented. The underlying data structure is an array. There is also a variable to keep track of the size, from which we can easily get the last occupied position in the array. Adding to the end just increases the size by 1 and assigns the object added to the next position. However, what happens when the array is full? A new array is allocated and everything is copied to the new array. Doing so takes time <span class="math">Θ(<em>n</em>)</span>, where <span class="math"><em>n</em></span> is the number of elements in the <code>ArrayList</code>.</p>
<p>If we had to copy the entire <code>ArrayList</code> upon each <code>add</code> operation, the process would be very slow. It would in fact take time <span class="math"><em>O</em>(<em>n</em><sup>2</sup>)</span> to add <span class="math"><em>n</em></span> elements to the end of the <code>ArrayList</code>. That would be too slow. So instead, when the <code>ArrayList</code> is full, the new array allocated is not just one position bigger than the old one, but much bigger. One option is to double the size of the array. Then a lot of <code>add</code> operations can happen before the array needs to be copied again.</p>
<p>With this approach, <span class="math"><em>n</em></span> <code>add</code> operations will take <span class="math"><em>O</em>(<em>n</em>)</span> time. In other words, the average time per operation is only <span class="math"><em>O</em>(1)</span>. We call this the <strong>amortized time</strong>. Amortization is what accountants do when saving up to buy an expensive item like a computer. Suppose that you want to buy a computer every 3 years and it costs 1500 dollars. One way to think about this is to have no cost the first two years and 1500 dollars the third year. An alternative is to set aside 500 dollars each year. In the third year you can take the accumulated 1500 dollars and spend it on the computer. So the computer costs 500 dollars a year, amortized. (In tax law it goes the other direction: you spend the 1500 dollars, but instead of being able to deduct the whole thing the first year you have to spead it over the 3 year life of the computer, and you deduct 500 dollars a year.)</p>
<p>For the <code>ArrayList</code> case, we can think in terms of tokens that can pay for copying something into the array. Suppose that we have just doubled the array size from <span class="math"><em>n</em></span> to <span class="math">2<em>n</em></span>, which means that we have just copied <span class="math"><em>n</em></span> elements; let's call these <span class="math"><em>n</em></span> elements that were copied the "old elements." We spend all our tokens copying the old elements, so that immediately after copying, we have no tokens available. By the time the array has <span class="math">2<em>n</em></span> elements and the array size doubles again, we must have one token for each of the <span class="math">2<em>n</em></span> elements to pay for copying <span class="math">2<em>n</em></span> elements.</p>
<p>Here's how we do it. We charge three tokens for each <code>add</code>:</p>
<ul>
<li>One token pays for adding the element to the array.</li>
<li>One token stays with this element to pay for the first time it's copied.</li>
<li>One token goes to one of the <span class="math"><em>n</em></span> old elements.</li>
</ul>
<p>Therefore, by the time the array has <span class="math">2<em>n</em></span> elements, every element has a token, which pays for copying it.</p>
<p>By thinking about it in this way, we see that the cost per <code>add</code> operation is a constant (three tokens).</p>
<p>The array size does not have to double when the array fills. For example, it could increase by a factor of <span class="math">3/2</span>, in which case you can modify the argument to show that charging four tokens per <code>add</code> operation works.</p>
<h2 id="queues">Queues</h2>
<p>A <code>Queue</code> is a FIFO (First In, First Out) data structure. "Queueing up" is a mostly British way of saying standing in line. And a <code>Queue</code> data structure mimics a line at a grocery store or bank where people join at the back, are served when they get to the front, and nobody is allowed to cut into the line. The ADT <code>Queue</code> has at at least the following operations (in addition to a constructor to create an empty <code>Queue</code>):</p>
<ul>
<li><code>enqueue</code>: add an element at the rear of the queue</li>
<li><code>dequeue</code>: remove and return the first element in the queue</li>
<li><code>front</code>: return the first element but don't remove it</li>
<li><code>isEmpty</code>: return a boolean indicating whether the queue is empty</li>
</ul>
<p>What do we use a <code>Queue</code> for? An obvious answer is that it is useful in simulations of lines at banks, toll booths, etc. But more important are the queues within computer systems for things like printers. When you submit a print job you are enqueued. When the print job gets to the front of the queue, it is dequeued and printed. Time-sharing systems use round-robin scheduling. The first job is dequeued and run for a fixed period of time or until it blocks (i.e., has to wait) for I/O or some other reason. Then it is enqueued. This process repeats as long as there are jobs in the queue. New jobs are enqueued. Jobs that finish leave the system instead of being enqueued. In this way, every job gets a fair share of the CPU. The book shows how to solve the Josephus problem using a queue. It is basically a round-robin scheduler, where every <span class="math"><em>k</em></span>th job is killed instead of being enqueued again.</p>
<p>A queue can also be used to search a maze. The same process is used as for the stack, but with a queue as the ADT. This leads to breadth-first search, and will find the shortest path through the maze. <!-- Demonstrate. --></p>
<h3 id="implementing-a-queue">Implementing a queue</h3>
<p>An obvious way to implement a queue is to use a linked list. A singly linked list suffices, if it includes a tail pointer. Enqueue at the tail and dequeue from the head. All operations take <span class="math">Θ(1)</span> time.</p>
<p>If you use a circular, doubly linked list with a sentinel, you can organize the list the opposite way: enqueue at the head and dequeue from the tail. If you were to try it this way for a singly linked list, you would keep having to run down the entire list to find the predecessor to the tail when dequeuing, and so this operation would take <span class="math">Θ(<em>n</em>)</span> time.</p>
<p>The textbook presents a <code>Queue</code> interface and part of an implementation using a singly linked list. They also include a <code>size</code> method. The interface <code>CS10Queue</code> in <a href="CS10Queue.java">CS10Queue.java</a> has the methods given above, and <a href="LinkedListQueue.java">LinkedListQueue.java</a> is an implementation that uses a <code>SentinelDLL</code>. It could be changed to use an <code>SLL</code> by changing one declaration and one constructor call. All operations would still take <span class="math">Θ(1)</span> time.</p>
<p>Java also has a <code>Queue</code> interface. It does not use the conventional names. Instead of <code>enqueue</code> it has <code>add</code> and <code>offer</code>. Instead of <code>front</code> it has <code>element</code> and <code>peek</code>. Instead of <code>dequeue</code> it has <code>remove</code> and <code>poll</code>. Why two choices for each? The first choice in each pair throws an exception if it fails. The second fails more gracefully, returning <code>false</code> if <code>offer</code> fails (because the queue is full) and <code>null</code> if <code>peek</code> or <code>poll</code> is called on an empty queue. At least <code>isEmpty</code> and <code>size</code> keep their conventional names.</p>
<h2 id="deques">Deques</h2>
<p>A deque (pronounced "deck") is a double-ended queue. You can add or delete from either end. A minimal set of operations is</p>
<ul>
<li><code>addFirst</code>: add to the head</li>
<li><code>addLast</code>: add to the tail</li>
<li><code>removeFirst</code>: remove and return the first element</li>
<li><code>removeLast</code>: remove and return the last element</li>
<li><code>isEmpty</code>: return a boolean indicating whether the deque is empty</li>
</ul>
<p>Additional operations include</p>
<ul>
<li><code>getFirst</code>: return the first element but do not remove it</li>
<li><code>getLast</code>: return the last element but do not remove it</li>
<li><code>size</code>: return the number of elements in the deque</li>
</ul>
<p>A deque can be used as a stack, as a queue, or as something more complex. In fact, the Java documentation recommends using a <code>Deque</code> instead of the legacy <code>Stack</code> class when you need a stack. This is. because the <code>Stack</code> class, which extends the <code>Vector</code> class, includes non-stack operations (e.g. searching through the stack for an item). (<code>Vector</code> was replaced by <code>ArrayList</code> and is deprecated in recent Java releases.)</p>
<h3 id="implementing-a-deque">Implementing a deque</h3>
<p>Implement a dequeue is with a <code>SentinelDLL</code>. If you look at the methods you will see that all of these operations are already there except for the two remove operations and <code>size()</code>. The remove operations can be implemented by calling either <code>getFirst</code> or <code>getLast</code> and then calling <code>remove</code>. The <code>size</code> operation can be left out or can be implemented by adding a <code>count</code> instance variable to keep track of the number of items in the deque. Each of these operations requires <span class="math">Θ(1)</span> time.</p>
<p>Once again, Java provides two versions of each of each deque operation. The two "add" operations have corresponding "offer" operations (<code>offerFirst</code> and <code>offerLast</code>). The two "remove" operations have corresponding "poll" operations, and the two "get" operations have corresponding "peek" operations. These alternate operations do not throw exceptions.</p>


